{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "efbf3fd3-a719-4163-9e70-d563a0e4eb1f",
   "metadata": {},
   "source": [
    "### Анализ текстов джазовых песен"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04f37294-b3e9-4f25-bdf5-ba05fb8b244e",
   "metadata": {},
   "source": [
    "**Описание**:\n",
    "\n",
    "В качестве материала для анализа мы решили взять песни четырех \"классических\" исполнителей музыки в жанре \"джаз\": Фрэнка Синатры, Дина Мартина, Нэта Кинг Коула и Луи Армстронга. Мы взяли по пятьдесят наиболее знаменитых и популярных песен каждого из композиторов (в случае Нэта Кинг Коула вышло только шестнадцать) для анализа по таким параметрам, как продолжительность и наиболее часто используемые слова.\n",
    "\n",
    "В качестве инструментов для парсинга текста был использован модуль lyricsgenius, продолжительность вытащена с Youtube."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "587f8bb0-c7c8-4e91-8381-eed2abb5df33",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -r requirements.txt\n",
    "!python -m spacy download en_core_web_sm\n",
    "import json\n",
    "import lyricsgenius\n",
    "import re\n",
    "import time\n",
    "import spacy\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from lyricsgenius import Genius\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "from collections import defaultdict, Counter\n",
    "from nltk import FreqDist\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from wordcloud import WordCloud"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c7b3f7c-fb64-4373-ab47-a4e3d9d88696",
   "metadata": {},
   "source": [
    "# Загрузка данных"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e58652b3-6dcc-4786-93cb-fa2980d785ff",
   "metadata": {},
   "source": [
    "Здесь представлено два варианта загрузки данных: \n",
    "* Облегченный - рекомендуемый метод, в котором мы используем предварительно сгенерированную системой и сохраненную на системе базу данных.\n",
    "* Оригинальный - скомпилированный воедино код из трех основных скриптов, скачивающий песни с Genius, собирающий их длительность с YouTube и год выпуска с Discogs. Время от времени api может выдавать ошибку на стороне сервера. Метод оставлен для демонстрации изначальной работы кода для генерации датабазы, однако наш анализ ведется на основе полной базы данных, собранной в файле.\n",
    "\n",
    "Выберите один из двух типов, запустите его ячейку, и продолжайте ниже."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2f2562f-ecdb-4bec-b784-2104be8f7e0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Облегченный метод, позволяющий пропустить ошибки api при генерации очищенного текста. Скачивает final_lyrics,\n",
    "#успешно прогнанный через все скрипты без ошибок.\n",
    "df = pd.read_csv('data/final_lyrics')\n",
    "#начальная предобработка\n",
    "df['lyrics_clean'] = df['lyrics'].replace(r'[^\\w\\s]',' ',regex=True).replace(r'\\s+',' ',regex=True).str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "870304f6-3982-4b0b-a072-1806830a4fd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Оригинальный код через скрипт. Время от времени api выдает ошибку.\n",
    "\n",
    "def clean_genius_lyrics(text):\n",
    "    # Удаляем все технические блоки перед текстом песни\n",
    "    text = re.sub(\n",
    "        r'^(\\d+\\s*Contributors|.*?Lyrics\\b).*?\\n',\n",
    "        '',\n",
    "        text,\n",
    "        flags=re.IGNORECASE | re.DOTALL | re.MULTILINE\n",
    "    )\n",
    "\n",
    "    # Удаляем Read More и всё до него\n",
    "    text = re.split(r'Read More\\s*\\n', text, flags=re.IGNORECASE)[-1] #Оставляем часть после \"Read More\"\n",
    "\n",
    "    # Дополнительные паттерны\n",
    "    patterns = [\n",
    "        r'Genius Annotation.*',\n",
    "        r'You might also like.*',\n",
    "        r'Embed\\s*\\d+',\n",
    "        r'\\xa0',\n",
    "        r'^[\\W\\d_]+$'  # любые не-буквенные символы, цифры или подчеркивания.\n",
    "    ]\n",
    "\n",
    "    for pattern in patterns:\n",
    "        text = re.sub(pattern, '', text, flags=re.IGNORECASE | re.DOTALL)\n",
    "\n",
    "    # Финальная очистка\n",
    "    text = \"\\n\".join([line.strip() for line in text.split(\"\\n\") if line.strip()]) # удаляем пустые строки и лишние пробелы и соединяем полученные строки\n",
    "    return text.strip()\n",
    "    \n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "genius = lyricsgenius.Genius(os.getenv('GENIUS_TOKEN'))\n",
    "genius.remove_section_headers = True # удаляем секции вида \"[Chorus], [Verse n]\" и похожего типа\n",
    "artists = ['Frank Sinatra', 'Dean Martin', 'Nat \"King\" Cole', 'Louis Armstrong']\n",
    "\n",
    "all_songs = []\n",
    "\n",
    "for artist in artists:\n",
    "    try:\n",
    "        if artist != 'Nat \"King\" Cole':\n",
    "            artist_obj = genius.search_artist(artist, max_songs = 50, sort='popularity')\n",
    "            for song in artist_obj.songs:\n",
    "                all_songs.append({\n",
    "                    'artist': song.artist,\n",
    "                    'title': song.title,\n",
    "                    'lyrics': clean_genius_lyrics(song.lyrics) # очищаем текст песни от лишних символов\n",
    "            })\n",
    "        else:\n",
    "            artist_obj = genius.search_artist(artist, max_songs=16, sort='popularity')\n",
    "            for song in artist_obj.songs:\n",
    "                all_songs.append({\n",
    "                    'artist': song.artist,\n",
    "                    'title': song.title,\n",
    "                    'lyrics': clean_genius_lyrics(song.lyrics) # очищаем текст песни от лишних символов\n",
    "                })\n",
    "    except:\n",
    "        print(f'Ошибка для {artist}')\n",
    "\n",
    "lyrics = pd.DataFrame(all_songs)\n",
    "lyrics = lyrics.drop_duplicates(subset=['artist', 'title', 'lyrics'])  # удаляем возможные дубликаты\n",
    "lyrics_csv = lyrics.to_csv('lyrics_all.csv',sep=',', index= False)\n",
    "\n",
    "# Получаем продолжительность\n",
    "\n",
    "with open(\".venv/config.json\") as f:\n",
    "    config = json.load(f)\n",
    "\n",
    "lyrics = pd.read_csv('lyrics_all.csv')\n",
    "def get_youtube_duration(artist, title):\n",
    "    try:\n",
    "        results = YoutubeSearch(f\"{artist} {title}\", max_results=1).to_dict()\n",
    "        return results[0][\"duration\"]\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "lyrics[\"duration\"] = lyrics.apply(lambda row: get_youtube_duration(row[\"artist\"], row[\"title\"]), axis=1)\n",
    "def convert_duration(duration):\n",
    "    minutes = int(duration)\n",
    "    seconds = round((duration - minutes) * 100)  # Извлекаем две цифры после точки\n",
    "    return (minutes * 60)  + seconds\n",
    "\n",
    "# Применяем функцию к колонке duration\n",
    "lyrics[\"duration\"] = lyrics[\"duration\"].apply(convert_duration)\n",
    "\n",
    "# Сохраняем результат в новый файл\n",
    "lyrics.to_csv(\"lyr_dur1_sec.csv\", index=False)\n",
    "\n",
    "#Загрузим необходимый токен для доступа к сайту\n",
    "\n",
    "DISCOGS_TOKEN = os.getenv('DISCOGS_TOKEN')\n",
    "\n",
    "#Получим год релиза песни\n",
    "def get_discogs_year(artist, title):\n",
    "    url = f\"https://api.discogs.com/database/search?q={artist}+{title}&type=release\"\n",
    "    headers = {\"Authorization\": f\"Discogs token={DISCOGS_TOKEN}\"}\n",
    "\n",
    "    try:\n",
    "        response = requests.get(url, headers=headers)\n",
    "        data = response.json()\n",
    "        return data[\"results\"][0][\"year\"] if data[\"results\"] else None\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "lyrics = pd.read_csv(\"data/lyr_dur1_sec.csv\")\n",
    "lyrics[\"year\"] = lyrics.apply(lambda row: get_discogs_year(row[\"artist\"], row[\"title\"]), axis=1)\n",
    "\n",
    "final_lyrics = lyrics.to_csv('final_lyrics', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c24d2204-c15b-49b4-a870-bf83fe5d98c9",
   "metadata": {},
   "source": [
    "## Очистка данных и подготовка к анализу"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03e52024-e95e-4ec0-830e-c721ddff1506",
   "metadata": {},
   "source": [
    "В данном разделе мы проведем исключительно разведочный анализ данных (EDA), поскольку данные были предварительно очищены в ходе изначальной генерации датасета в коде выше, и достать чисто этот функционал для раздела не представляется возможным.\n",
    "\n",
    "Начнем с рассмотрения самого датасета и выясним, из каких колонок он состоит."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb33b863-b25f-4b91-a214-47d45aca1e0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50996e0b-4ee1-4711-a0ff-cbf6cda93858",
   "metadata": {},
   "source": [
    "Здесь мы можем пронаблюдать следующие колонки:\n",
    "\n",
    "* **artist** - имя артиста;\n",
    "* **title** - название песни;\n",
    "* **lyrics** - необработанный текст песни, не подходящий для лемматизации и анализа;\n",
    "* **duration** - продолжительность песни (в секундах);\n",
    "* **year** - год выпуска;\n",
    "* **lyrics_clean** - очищенный вариант текста песни, подходящий для лемматизации;\n",
    "\n",
    "Теперь давайте посмотрим, к каким типам данных принадлежат данные колонки:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "218ecdc7-4141-499c-9ea2-768161c0f741",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cb9a564-59ba-4d60-bbcc-0e4e75289091",
   "metadata": {},
   "source": [
    "* Здесь мы можем пронаблюдать, что **artist, title** и **lyrics**(а также **lyrics_clean**) принадлежат к типу **object**. Соответственно, **year** принадлежит к типу **float64**, а **duration** - к типу **int64**.\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a99263d-623a-4daa-ac81-2e35dea3e6d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "no_artist = df['artist'].isna()\n",
    "no_title = df['title'].isna()\n",
    "no_lyrics = df['lyrics'].isna()\n",
    "no_duration = df['duration'].isna()\n",
    "no_year = df['year'].isna()\n",
    "\n",
    "print(f\"Количество строк с отсутствующим именем артиста: {no_artist.sum()}\")\n",
    "print(f\"Количество строк с отсутствующим заголовком: {no_title.sum()}\")\n",
    "print(f\"Количество строк с отсутствующим текстом: {no_lyrics.sum()}\")\n",
    "print(f\"Количество строк с отсутствующей продолжительностью трека: {no_duration.sum()}\")\n",
    "print(f\"Количество строк с отсутствующим годом выхода: {no_year.sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f79bff29-1795-4b7c-8aa9-f4b0c8c5b287",
   "metadata": {},
   "source": [
    "* Учитывая количество пропусков по датам выхода треков, мы не можем проводить анализ значения year, так как это более половины всех треков в нашей выборке. Мы не рассматриваем **lyrics_clean** в анализе данных, поскольку результат будет идентичен **lyrics**, учитывая, что **lyrics_clean** является его производной.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7679a0c4-b021-47a6-b7ab-09a1e6a315ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Загрузим модель для английского языка\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Исключаем из подборки всевозможные напевы, не являющиеся реальными словами.\n",
    "custom_stop_words = [\"gobble\", \"bidee\", \"ve\", \"ll\", \"oh\", \"ya\", \"ain\", \"don\", \"didn\", \"te\",\n",
    "\"fasule\", \"tay\", \"ce\", \"la\", \"em\", \"di\", \"il\", \"er\", \"de\", \"to\", \"ba\", \"da\", \"doo\", \"zee\", \"boo\", \"mmm\"]\n",
    "\n",
    "# Добавим кастомные слова к стоп-словам\n",
    "STOP_WORDS.update(custom_stop_words)\n",
    "\n",
    "def lemmatize_text(text):\n",
    "    doc = nlp(text)\n",
    "    lemmas = []\n",
    "    for token in doc:\n",
    "        if not token.is_stop and token.is_alpha and len(token) > 1:\n",
    "            lemmas.append(token.lemma_)\n",
    "    return lemmas\n",
    "\n",
    "# Применим лемматизацию к предобработанному столбцу с текстами песен\n",
    "df['lemmatized'] = df['lyrics_clean'].apply(lemmatize_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "415de5ad-3eb9-4de6-b63a-1080664d704e",
   "metadata": {},
   "source": [
    "## Анализ и визуализация"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a996004-be3b-4236-9c17-10c4ddba94de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Собираем все слова из колонки 'lemmatized'\n",
    "all_words = [word for row in df['lemmatized'] for word in row]\n",
    "\n",
    "# Обрабатываем текст через spaCy\n",
    "doc = nlp(\" \".join(all_words))\n",
    "\n",
    "# Создаем распределение по частям речи\n",
    "pos_distribution = defaultdict(list)\n",
    "\n",
    "for token in doc:\n",
    "    pos = token.pos_  # Универсальные теги (NOUN, VERB, ADJ, ADV и т.д.)\n",
    "    pos_distribution[pos].append(token.text)\n",
    "\n",
    "# Упрощаем теги\n",
    "simplified_mapping = {\n",
    "    \"NOUN\": \"noun\",\n",
    "    \"VERB\": \"verb\",\n",
    "    \"ADJ\": \"adj\",\n",
    "    \"ADV\": \"adv\",\n",
    "    # Остальные категории объединяем в 'other'\n",
    "}\n",
    "\n",
    "simplified_distribution = defaultdict(list)\n",
    "for pos, words in pos_distribution.items():\n",
    "    simplified_pos = simplified_mapping.get(pos, \"other\")\n",
    "    simplified_distribution[simplified_pos].extend(words)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6c7437b-6313-4798-83ca-830ddc1b924d",
   "metadata": {},
   "source": [
    "### Топ-20 распространенных слов по их типам"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3577c1c-1285-4f8a-a170-30ba1e6f2a30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Топ 20 существительных\n",
    "top_noun = FreqDist(simplified_distribution['noun']).most_common(20)\n",
    "print(top_noun)\n",
    "# Топ 20 глаголов\n",
    "top_verb = FreqDist(simplified_distribution['verb']).most_common(20)\n",
    "print(top_verb)\n",
    "# Топ 20 прилагательных\n",
    "top_adj = FreqDist(simplified_distribution['adj']).most_common(20)\n",
    "print(top_adj)\n",
    "# Топ 20 наречий\n",
    "top_adv = FreqDist(simplified_distribution['adv']).most_common(20)\n",
    "print(top_adv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e37a5e4a-d8bb-4c62-9b48-3b89d0770c97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Настройка стиля\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "# Функция для построения графиков\n",
    "def plot_top_words(data, title, palette, position):\n",
    "    plt.subplot(2, 2, position)\n",
    "    words, counts = zip(*data)\n",
    "    sns.barplot(\n",
    "        x=list(counts),\n",
    "        y=list(words),\n",
    "        hue=list(words),\n",
    "        palette=palette,\n",
    "        legend=False\n",
    "    )\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"Частота\")\n",
    "    plt.ylabel('Слова')\n",
    "\n",
    "# Построение графиков\n",
    "plot_top_words(top_noun, \"Топ-20 существительных\", \"Blues_d\", 1)\n",
    "plot_top_words(top_verb, \"Топ-20 глаголов\", \"Greens_d\", 2)\n",
    "plot_top_words(top_adj, \"Топ-20 прилагательных\", \"Reds_d\", 3)\n",
    "plot_top_words(top_adv, \"Топ-20 наречий\", \"Purples_d\", 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84dc493a-97fb-48db-a4e3-791c19f67b79",
   "metadata": {},
   "source": [
    "### Характерные для исполнителей слова"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95684669-81bc-4f57-b5ba-ddcec1caac3b",
   "metadata": {},
   "source": [
    "**Небольшая теоретическая справка про TF-IDF анализ TF** *(Term Frequency)*:\n",
    "* Относительная частота слова в документе: TF = (число вхождений слова в документе) / (общее число слов в документе).\n",
    "\n",
    "*IDF (Inverse Document Frequency)*:\n",
    "* Мера редкости слова в корпусе: IDF = log( (число документов) / (число документов, содержащих слово) ).\n",
    "\n",
    "TF-IDF:\n",
    "* TF * IDF — чем выше значение, тем важнее слово для документа в контексте всего корпуса."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6179773c-53ed-4056-81a8-e248c7523578",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Объединим леммы в строки\n",
    "df['lemmatized_text'] = df['lemmatized'].apply(lambda x: ' '.join(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "470d2caa-1ae0-4b08-b14a-9dcf4bf2e64f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Группировка по исполнителям и объединение в одну строку\n",
    "grouped = df.groupby('artist')['lemmatized_text'].agg(' '.join).reset_index()\n",
    "\n",
    "# Создание TF-IDF матрицы\n",
    "vectorizer = TfidfVectorizer(max_features=1000, stop_words = list(STOP_WORDS))   # Не забываем про расширенный набор стоп-слов\n",
    "tfidf_matrix = vectorizer.fit_transform(grouped['lemmatized_text'])\n",
    "\n",
    "# Датафрейм с весами TF-IDF\n",
    "tfidf_df = pd.DataFrame(\n",
    "    tfidf_matrix.toarray(),\n",
    "    columns=vectorizer.get_feature_names_out(),\n",
    "    index=grouped['artist']\n",
    ")\n",
    "\n",
    "# Функция для извлечения топ-слов\n",
    "def get_top_artist_words(artist, n=10):\n",
    "    return tfidf_df.loc[artist].nlargest(n).to_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61b2706b-6fd9-4ac6-9e43-922ed0cf8971",
   "metadata": {},
   "source": [
    "### Визуализируем характерные слова для каждого из артистов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45941671-d75f-4cc3-80b9-c558c44dcc08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Настройка стиля\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "plt.figure(figsize=(20, 15))  # Общий размер для всех субграфиков\n",
    "\n",
    "# Создаем сетку 2x2 для 4 графиков\n",
    "plt.subplots_adjust(hspace=0.4, wspace=0.4) # Свободное пространство между графиками\n",
    "\n",
    "def plot_artist_tfidf(artist, palette, position, n=10):\n",
    "    plt.subplot(2, 2, position)\n",
    "    top_words = tfidf_df.loc[artist].nlargest(n) # количество максимально встречающихся слов\n",
    "\n",
    "    # параметры barplot\n",
    "    sns.barplot(\n",
    "        x=top_words.values,           # Выцепим вес tf-idf\n",
    "        y=top_words.index,            # Выцепим слово\n",
    "        hue=top_words.index,\n",
    "        palette=palette,\n",
    "        legend=False,\n",
    "        dodge=False                   # Выключаем наложение баров/столбцов\n",
    "    )\n",
    "\n",
    "    plt.title(f\"Топ-{n} характерных слов для {artist}\", fontsize=12)\n",
    "    plt.xlabel(\"Вес TF-IDF\", fontsize=10)\n",
    "    plt.ylabel(\"Слова\")\n",
    "    plt.tick_params(axis='y', labelsize=9)\n",
    "\n",
    "# Рисуем все графики на одной фигуре\n",
    "plot_artist_tfidf('Nat “King” Cole', 'YlOrBr', 1)\n",
    "plot_artist_tfidf('Frank Sinatra', 'icefire', 2)\n",
    "plot_artist_tfidf('Dean Martin', 'Spectral', 3)\n",
    "plot_artist_tfidf('Louis Armstrong', 'coolwarm', 4)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "696f7516-5589-4624-b135-77eda88e7b0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def generate_wordcloud(artist_name=None, df=None,\n",
    "                       background_color='white',\n",
    "                       colormap=None,\n",
    "                       colormap_list=['viridis', 'plasma', 'magma', 'cividis', 'cool', 'twilight_shifted', 'icefire'],\n",
    "                       max_words=100,\n",
    "                       figsize=(12, 8)):\n",
    "    \"\"\"\n",
    "    Генерирует облако слов со случайной цветовой схемой из списка.\n",
    "\n",
    "    Параметры:\n",
    "    - artist_name: Имя исполнителя, если не указано выводится облако слов по всем исполнителям\n",
    "    - colormap_list: список допустимых цветовых схем.\n",
    "    - colormap: если указан, будет использован вместо случайного выбора.\n",
    "    \"\"\"\n",
    "    # Выбор цветовой схемы\n",
    "    if colormap is None:\n",
    "        if colormap_list:\n",
    "            colormap = random.choice(colormap_list)\n",
    "        else:\n",
    "            colormap = 'viridis'  # Дефолтное значение\n",
    "    # Фильтрация данных\n",
    "    if artist_name:\n",
    "        data = df[df['artist'] == artist_name]['lemmatized_text'] # Фильтр по исполнителю\n",
    "        if data.empty:\n",
    "            raise ValueError(f\"Исполнитель '{artist_name}' не найден в данных.\")\n",
    "    else:\n",
    "        data = df['lemmatized_text']\n",
    "\n",
    "    # Объединение текстов в одну строку\n",
    "    all_text = ' '.join(data.astype(str))\n",
    "\n",
    "    # Подсчет частоты слов\n",
    "    word_freq = Counter(all_text.split())\n",
    "\n",
    "    # Генерация облака\n",
    "    wordcloud = WordCloud(\n",
    "        width=figsize[0] * 100,                 # настроим ширину фигуры\n",
    "        height=figsize[1] * 100,                # настроим высоту фигуры\n",
    "        background_color=background_color,      # заливка фона\n",
    "        colormap=colormap,                      # выбор цветовой палитры\n",
    "        max_words=max_words                     # максимальное количество слов\n",
    "    ).generate_from_frequencies(word_freq)      # сгенирируем облако на основе частот встречаемости\n",
    "\n",
    "    # Визуализация\n",
    "    plt.figure(figsize=figsize)\n",
    "    plt.imshow(wordcloud, interpolation='bilinear') # отобразим облако со сглаживанием\n",
    "    plt.axis('off')\n",
    "    if artist_name:\n",
    "        plt.title(f'Облако слов для {artist_name}', fontsize=14, pad=20)\n",
    "    else:\n",
    "        plt.title('Облако слов для всех исполнителей', fontsize=14, pad=20)\n",
    "    plt.show()\n",
    "\n",
    "# Пример использования для Дина Мартина\n",
    "generate_wordcloud(artist_name='Frank Sinatra', df=df)\n",
    "# Для всех исполнителей\n",
    "generate_wordcloud(df=df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "959bca5a-bc93-44ff-a973-a33b100d30d3",
   "metadata": {},
   "source": [
    "## Выводы "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a2284ed-626d-49a3-9deb-392935e0bd31",
   "metadata": {},
   "source": [
    "### **Анализ частоты слов**:\n",
    "\n",
    "Чаще всего именно существительные являются центральными словами в песнях, поскольку именно существительные выступают темой и главным лейтмотивом тех или иных песен. Как ни странно и предсказуемо, именно \"love\" (любовь) лидирует в топ-20 слов среди существительных. Будучи одной из главных тем песен любой эпохи, в эпоху джаза любовь остается самым распространенным словом - свыше 200 упоминаний среди существительных. Это также самое распространенное слово в джазовых песнях в целом.\n",
    "\n",
    "**Однако!** Не стоит забывать про то, что \"love\" - также и глагол. Пускай это и не отменяет остальных наблюдений, даже так \"love\" имеет вдвое больше упоминаний, чем \"way\", как второй глагол. Вместе с существительным вариантом, \"love\" является САМЫМ распространенным словом среди всех четырех топов.\n",
    "\n",
    "Из прилагательных - стоит отметить, что почти все они, кроме двух (low, cold) имеют так или иначе положительный окрас. Наличие этих двоих слов в топе, учитывая их относительно небольшое количество (около 15 у каждого) может либо значить использование подобных слов в качестве контраста в песнях, либо то, что они встречаются лишь в небольшом количестве песен. В остальном, положительный окрас большинства прилагательных можно связать с двумя вещами: либо то, что как жанр, сам по себе джаз в основном поднимает темы с положительным окрасом, вроде любви, счастья, верности, и сами песни поэтому имеют положительный окрас. Либо это можно связать с социокультурным явлением послевоенной \"золотой эпохи\" и экономическим процветанием США. Закат эры свинга как музыкального жанра пришелся на первую половину сороковых, в то время как джаз захватил популярность и стал главным музыкальным жанром США как раз к второй половине сороковых и оставался таким вплоть до шестидесятых. \n",
    "\n",
    "В анализе глаголов стоит отметить глагол \"let\" - в то время, как сами поставленные нами условия это не нарушает, одной из двух пересекающихся песен, помимо \"Blue Moon\", является \"Let it Snow\" с 25 упоминаниями глагола \"let\". Учитывая известность обоих вариантов этих песен, как и в случае с \"Blue Moon\", для общего проекта это не является особо большой проблемой. В остальном глаголы и наречания не представляют особого интереса, и особо ничего не сообщают.\n",
    "\n",
    "### **Анализ частоты слов**:\n",
    "\n",
    "Данные столбчатые диаграммы демонстрируют частоту упоминания слов по песням каждого из отдельно взятых исполнителей на основе TF-IDF. Самым интересным фактом, пожалуй, следует отметить, что для трех из четырех исполнителей самым характерным и часто используемым остается слово \"love\", любовь. Не важно, используется оно в роли существительного или глагола, однако встречается оно значительно чаще других.\n",
    "\n",
    "И только Луи Армстронг предпочитает любви вкусные чизкейки - вес этого слова у него выше 0.40, что, по меркам IDF, весьма много. Единственный, у кого главное слово имеет схожий вес - Нэт Кинг Коул. В частности, можно заметить, что вес у \"love\" в его случае также заметно выше 0.40 и ближе к 0.50, однако тут следует учитывать саму формулу TF-IDF, а также то, что в его случае выборка в почти три раза меньше, чем у остальных музыкантов, и анализ не может считаться полностью правдоподобным и корректным.\n",
    "\n",
    "Отдельно также выделяется Дин Мартин - в топе его характерных слов встречаются не только напевы \"bon\" и \"boom\", но и иностранное \"si\", присутсвовавшее в двух песнях, \"C'est si bon\", и \"Mambo Italliano\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efe8a978-a5ce-4254-9eac-1e9df846a2d5",
   "metadata": {},
   "source": [
    "## Обсуждение"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6740d148-2b55-4900-a8b7-94805e431a2d",
   "metadata": {},
   "source": [
    "**Что мы хотели сделать в рамках нашего исследования и что сделать удалось**\n",
    "\n",
    "Изначально нашим планом было проанализировать лишь три исполнителя, однако в процессе мы решили добавить четвертого (Луи Армстронга) для более широкой выборки. Это также позволило нам компенсировать тот факт, что у Нэта Кинг Коула лишь шестнадцать песен вместо пятидесяти, как у всех остальных. Можно считать, что самую основную часть работы (выяснение продолжительности песен, анализ наиболее часто встречаемых слов) мы выполнили на \"отлично\".\n",
    " \n",
    "**Что не удалось сделать и почему**\n",
    "\n",
    "Одной из главных визуализаций данных, которую мы планировали вставить, изначально был анализ наиболее значимых для этих музыкантов лет через подсчет песен по датам их выпуска и создания на этой основе графика. Проблема возникла с музыкальной базой данных и её api - из 166 треков 90 оказалось без дат. Следовательно, анализ данных получился бы по меньшей мере практически полностью недостоверным.  Мы также были вынуждены отказаться от полноценной визуализации с wordcloud в связи с некоторыми багами, из-за которых он мог цеплять целые словосочетания вместо одного слова. В один момент мы также рассматривали идею убрать дубликаты песен из подборки, однако в конечном счете решили, что это не имеет смысла относительно того, что именно мы анализируем. \n",
    "\n",
    "**Как наше исследование можно было бы улучшить**\n",
    "\n",
    "Возможно, расширить выборку ещё на несколько знаменитых джазовых исполнителей: возможно, Пегги Ли, Бинга Кросби, Джона Колтрэйна, Билли Холидея, Сэмми Дэвиса. Если бы мы расширили выборку, из нишевого проекта это стало бы более значимой работой. Кроме того, вместо 50 наиболее знаменитых песен каждого музыканта мы могли взять всю их дискографию. Однако в таком случае также пришлось бы чистить подборку от возможных дубликатов некоторых песен, вроде Blue Moon. Из более экспериментальных идей - попробовать пошарить по американским сайтам в поисках данных о продажах копий джазовых альбомов Фрэнка Синатры, как одного из самых известных исполнителей джазовых песен (даже если и не джазмена в классическом понимании термина) и сопоставить с продажами рок-н-ролльных альбомов и синглов Элвиса Пресли, как самого известного исполнителя в жанре рок-н-ролла, дабы отследить момент, где джаз уступил рок-н-роллу в популярности среди простых обывателей.\n",
    "\n",
    "**Fun Fact:**\n",
    "В нашем готовом датасете (пусть и сделанном на основе скрипта) песня \"Blue Moon\" встречается два раза. Изначально не написанная ни Синатрой, ни Дином Мартином, она была создана в тридцатых и исполнялась огромным количеством певцов и групп в самые разные времена. Её исполнял даже Элвис Пресли в самой начале своей карьеры, причем его звучание песни сильно отличается не только от более торжественных версий, но и от его более поздних работ.\n",
    "\n",
    "**Кому наше исследование может быть полезно (или что можно сделать дополнительно, чтобы оно было полезным)**\n",
    "\n",
    "Если провести изменения, которые мы описали в прошлом блоке по поводу улучшения нашего проекта, чисто теоретически, оно бы могло использоваться в полноценной научной работе по поводу джаза. Провести подобное с рок-н-роллом в тех же годах, а также с ещё парой доминирующих жанров, вроде диско и более классического рока/рэпа, и подобную таблицу можно было бы использовать в научной работе по поводу различных музыкальных эпох в США, и как одни жанры выходили из моды и заменялись другими с развитием культуры. В зависимости от типа исследования также может иметь смысл удаление дубликатов Blue Moon и Let it Snow.\n",
    "\n",
    "В общем и целом, это исследование так и останется весьма специфичным и нишевым исследованием, однако базовая логика может быть улучшена и адаптирована и для более серьезных работ."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
